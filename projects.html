<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>projects_GK</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Dosis:wght@300&display=swap" rel="stylesheet">
</head>
<body>
  <div class="project">
    <div class="#">
        <h2>My projects.</h2>
        <br>
        <div class="#`">
          <img class="robo1" src="images/robo.png" alt="robo_icon">
          <h3><a href="https://github.com/Dhruv454000/Gesture-detection-and-replication">Gesture Detection and Replication</a> </h3>
          <p>OpenCv, Python, Coppeliasim</p>
          <ul>
            <li>Aim of the project was to detect and recognise basic hand gestures and imitate them using a simulation
              of a robotic hand.</li>
            <li>We have tested many methods for gesture detection but our main focus was on Convexity defects and CNN
              model to detect gestures</li>
            <li>Used Convexity defects and Contours for gesture detection and simulation was done on CoppeliaSim.</li>
            <li>We have used Remote API functions of coppeliasim for connection with coppeliasim. Depending on the
              gesture detected movement was done by robotic hand.</li>
          </ul>
          <hr>
        </div>
        <div class="#">
          <img class="robo2" src="images/robo.png" alt="#">
          <h3><a href="https://github.com/SRA-VJTI/Wall-E">Wall-E</a></h3>
          <p>IOT, ESP-IDF</p>
          <ul>
            <li>Aim of the project was to make a Self balancing and a line following bot</li>
            <li>we had used ESP-32 microcontroller for this project</li>
            <li>we updated the PID control values for line following over wifi on the bot.</li>
            
          </ul>
          <hr>
        </div>
       
      <br>
        <div class="#">
          <img class="robo3" src="images/robo.png" alt="#" >
          <h3><a href="https://github.com/SRA-VJTI/MARIO">MARIO</a></h3>
          <p>ROS, Gazebo, Rviz</p>
          <ul>
            <li>MARIO abbreviation for Manipulator on ROS Based Input Output is a bot with 3 Degree of Freedom. It
              consists of two SG90 micro servo and one MG995 metal gear servo motor</li>
            <li>The servo motors are placed on base, elbow and shoulder enabling it with 3 Degrees of Freedom.</li>
            
            
          </ul>

          <hr>
         
          
        </div>
        <br>

        <div class="#">
          <img class="robo4" src="images/robo.png" alt="#" >
          <h3>Autonomous Vehicle navigation</h3>
          <p>ROS, Gazebo, Rviz, CNN, opencv</p>
          <ul>
            <li>This project utilises the use of machine learning to drive the vehicle in a stimulation environment using machine
              learning algorithms.</li>
            <li>The stimulation is in gazebo enviornment and controlled by the ROS</li>
            
            
          </ul>
          
        </div>
        
        </div>
</body>
</html>